{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.9.1-cp36-cp36m-manylinux1_x86_64.whl (17.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.4 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from torchvision) (8.1.0)\n",
      "Collecting torch==1.8.1\n",
      "  Downloading torch-1.8.1-cp36-cp36m-manylinux1_x86_64.whl (804.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1 MB 112.7 MB/s eta 0:00:01   |████████████▏                   | 306.7 MB 3.3 MB/s eta 0:02:33     |████████████████████▌           | 516.1 MB 74.6 MB/s eta 0:00:04███▊        | 595.1 MB 79.6 MB/s eta 0:00:03██████████████████████▊       | 622.2 MB 79.6 MB/s eta 0:00:03��█████████████▍    | 688.4 MB 75.4 MB/s eta 0:00:02██████████████████▌   | 716.8 MB 75.4 MB/s eta 0:00:02 eta 0:00:02B/s eta 0:00:01MB 105.4 MB/s eta 0:00:01██████▏ | 758.4 MB 105.4 MB/s eta 0:00:01     |██████████████████████████████▋ | 767.9 MB 105.4 MB/s eta 0:00:01██████▊ | 771.4 MB 105.4 MB/s eta 0:00:01█▎| 786.8 MB 112.7 MB/s eta 0:00:01��██████████████████████████▉| 801.2 MB 112.7 MB/s eta 0:00:01Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from markdown) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown) (3.1.0)\n",
      "Installing collected packages: markdown\n",
      "Successfully installed markdown-3.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.1-cp36-cp36m-manylinux2010_x86_64.whl (394.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.3 MB 23 kB/s s eta 0:00:01     |██████████████████▏             | 224.0 MB 40.2 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 65.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 52.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 44.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 51.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.15.8-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 52.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 41.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 6.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.29.0-py2.py3-none-any.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 50.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 57.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (49.2.0.post20200712)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 47.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (1.7.0)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 41.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.0.1)\n",
      "Building wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69740 sha256=827a5c3d9e8bf3bda4f6a1c94d023f2554617de0cb552a52e4671748ac978ee9\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=fcbdd622264db031227c5f2bbddd551dbb446b536f453cd301053cb0b3fe46f0\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built wrapt termcolor\n",
      "Installing collected packages: h5py, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, wheel, werkzeug, protobuf, absl-py, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, grpcio, tensorboard, typing-extensions, tensorflow-estimator, astunparse, keras-preprocessing, google-pasta, flatbuffers, opt-einsum, gast, wrapt, termcolor, tensorflow\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.1 flatbuffers-1.12 gast-0.3.3 google-auth-1.29.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 opt-einsum-3.3.0 protobuf-3.15.8 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from keras) (1.5.3)\n",
      "Requirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.6/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: h5py in /srv/conda/envs/notebook/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.6/site-packages (from h5py->keras) (1.15.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --user --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getZerosAndOnes(fileName, electrod, trainslabel):\n",
    "    data = pd.read_csv(fileName)\n",
    "    session= data[[electrod,'FeedBackEvent']]\n",
    "    labels = pd.read_csv(trainslabel) \n",
    "    eventsIndexes=[]\n",
    "    for index, row in session.iterrows():\n",
    "        if row['FeedBackEvent']==1:\n",
    "            eventsIndexes.append(index)\n",
    "    eventsValues=[]\n",
    "    for index, row in labels.iterrows():\n",
    "        if row['IdFeedBack'].startswith('S02_Sess01'):\n",
    "            eventsValues.append(row['Prediction'])   \n",
    "    dataEpoches=[]\n",
    "    for (index, val) in enumerate(eventsIndexes):\n",
    "        if eventsValues[index]==1:\n",
    "            dataEpoches.append(session[val-120:val+80])\n",
    "    moreDataEpoches=[]\n",
    "    for (index, val) in enumerate(eventsIndexes):\n",
    "        if index>0 and index<len(eventsIndexes)-1:\n",
    "            count=1\n",
    "            while val+80+count*200<eventsIndexes[index+1]-120:\n",
    "                count+=1\n",
    "                moreDataEpoches.append(session[val+80+count*200:val+80+count*200+200])\n",
    "    Xm=[]\n",
    "    for row in moreDataEpoches:\n",
    "        Xm.append(row[electrod])\n",
    "    Xm=np.array(Xm)\n",
    "    Xs=[]\n",
    "    for row in dataEpoches:\n",
    "        Xs.append(row[electrod])\n",
    "    Xs=np.array(Xs)\n",
    "    y_dataZero=np.zeros(len(Xm))\n",
    "    y_dataOne=np.ones(len(Xs))\n",
    "    x_dataZero=Xm\n",
    "    x_dataOne=Xs\n",
    "    return x_dataOne, y_dataOne, x_dataZero, y_dataZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataOne, y_dataOne, x_dataZero, y_dataZero=getZerosAndOnes(\"data.csv\", 'Fp2', \"TrainLabels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 200)\n",
      "(50,)\n",
      "(491, 200)\n",
      "(491,)\n"
     ]
    }
   ],
   "source": [
    "print(x_dataOne.shape)\n",
    "print(y_dataOne.shape)\n",
    "print(x_dataZero.shape)\n",
    "print(y_dataZero.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_arrays(one, zero, y_one, y_zero, train_test_split=0.9):\n",
    "    data_size_one = len(one)\n",
    "    data_size_zero = len(zero)\n",
    "    data_size=data_size_one+data_size_zero\n",
    "    test_size = int(data_size - round(data_size_zero * train_test_split)- round(data_size_one * train_test_split))\n",
    "    print(\"Data size: {}\".format(data_size))\n",
    "    print(\"\\nTest size: {}\".format(test_size))\n",
    "    \n",
    "    data_one=one[0:round(data_size_one * train_test_split)]\n",
    "    data_zero=zero[0:round(data_size_zero * train_test_split)]\n",
    "    data_train=np.concatenate((data_one, data_zero), axis=0)\n",
    "    train=pd.DataFrame(data_train)\n",
    "    \n",
    "    print(data_one.shape)\n",
    "    print(data_zero.shape)\n",
    "    print(train.shape)\n",
    "    print(\"\\nTraining set:\")\n",
    "    x_train = train\n",
    "    \n",
    "    data_one=y_one[0:round(data_size_one * train_test_split)]\n",
    "    data_zero=y_zero[0:round(data_size_zero * train_test_split)]\n",
    "    data_train=np.concatenate((data_one, data_zero), axis=0)\n",
    "    train=pd.DataFrame(data_train)\n",
    "    \n",
    "    print(data_one.shape)\n",
    "    print(data_zero.shape)\n",
    "    print(train.shape)\n",
    "    print(\"\\t - x_train: {}\".format(len(x_train)))\n",
    "    y_train = train\n",
    "    print(\"\\t - y_train: {}\".format(len(y_train)))\n",
    "    \n",
    "    \n",
    "    data_one=one[round(data_size_one * train_test_split):data_size_one]\n",
    "    data_zero=zero[round(data_size_zero * train_test_split):data_size_zero]\n",
    "    data_test=np.concatenate((data_one, data_zero), axis=0)\n",
    "#     random.shuffle(data_test)\n",
    "    test=pd.DataFrame(data_test)\n",
    "    \n",
    "    print(data_one.shape)\n",
    "    print(data_zero.shape)\n",
    "    print(train.shape)\n",
    "    print(\"\\nTesting set:\")\n",
    "    x_test = test\n",
    "    \n",
    "    data_one=y_one[round(data_size_one * train_test_split):data_size_one]\n",
    "    data_zero=y_zero[round(data_size_zero * train_test_split):data_size_zero]\n",
    "    data_test=np.concatenate((data_one, data_zero), axis=0)\n",
    "#     random.shuffle(data_test)\n",
    "    test=pd.DataFrame(data_test)\n",
    "    \n",
    "    print(data_one.shape)\n",
    "    print(data_zero.shape)\n",
    "    print(train.shape)\n",
    "    print(\"\\t - x_test: {}\".format(len(x_test)))\n",
    "    y_test = test\n",
    "    print(\"\\t - y_test: {}\".format(len(y_test)))\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 541\n",
      "\n",
      "Test size: 135\n",
      "(38, 200)\n",
      "(368, 200)\n",
      "(406, 200)\n",
      "\n",
      "Training set:\n",
      "(38,)\n",
      "(368,)\n",
      "(406, 1)\n",
      "\t - x_train: 406\n",
      "\t - y_train: 406\n",
      "(12, 200)\n",
      "(123, 200)\n",
      "(406, 1)\n",
      "\n",
      "Testing set:\n",
      "(12,)\n",
      "(123,)\n",
      "(406, 1)\n",
      "\t - x_test: 135\n",
      "\t - y_test: 135\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test=load_data_from_arrays(x_dataOne, x_dataZero,y_dataOne,y_dataZero,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406, 200)\n",
      "(406, 1)\n",
      "(135, 200)\n",
      "(135, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(200,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "y = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='output')(y)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "..   ...\n",
       "130  0.0\n",
       "131  0.0\n",
       "132  0.0\n",
       "133  0.0\n",
       "134  0.0\n",
       "\n",
       "[135 rows x 1 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train = (np.array(x_train)).reshape(406, 200).astype('float32')\n",
    "# x_test = (np.array(x_test)).reshape(135, 200).astype('float32')\n",
    "# y_train = (np.array(y_train)).reshape(406, 1).astype('float32')\n",
    "# y_test = (np.array(y_test)).reshape(135, 1).astype('float32')\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Обучаем модель на тестовых данных\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 34ms/step - loss: 17.1672 - sparse_categorical_accuracy: 0.9125 - val_loss: 8.4486 - val_sparse_categorical_accuracy: 0.9111\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 7.0569 - sparse_categorical_accuracy: 0.9201 - val_loss: 6.1698 - val_sparse_categorical_accuracy: 0.9111\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 7.3639 - sparse_categorical_accuracy: 0.8904 - val_loss: 2.0970 - val_sparse_categorical_accuracy: 0.9111\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 4.8606 - sparse_categorical_accuracy: 0.9126 - val_loss: 6.4138 - val_sparse_categorical_accuracy: 0.9111\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3.0513 - sparse_categorical_accuracy: 0.9173 - val_loss: 9.4918 - val_sparse_categorical_accuracy: 0.9111\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "print('# Обучаем модель на тестовых данных')\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=5,\n",
    "                    # Мы передаем валидационные данные для\n",
    "                    # мониторинга потерь и метрик на этих данных\n",
    "                    # в конце каждой эпохи\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 9.4918 - sparse_categorical_accuracy: 0.9111\n",
      "test loss, test acc: [9.491764068603516, 0.9111111164093018]\n",
      "размерность прогнозов: (135, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "predictions = model.predict(x_test)\n",
    "print('размерность прогнозов:', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=np.round(predictions,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  [0.]\n"
     ]
    }
   ],
   "source": [
    "for (index, val) in enumerate(y_test):\n",
    "    print(val,\" : \",predictions[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]  :  [0.]\n",
      "[1.]  :  [5.5e-14]\n",
      "[0.]  :  [2.0379e-11]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [4.1e-14]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [5.e-15]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [5.380454e-09]\n",
      "[0.]  :  [1.961e-12]\n",
      "[0.]  :  [3.804e-12]\n",
      "[0.]  :  [1.e-15]\n",
      "[0.]  :  [5.149e-12]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [4.3644e-11]\n",
      "[0.]  :  [6.22334e-10]\n",
      "[0.]  :  [1.194e-12]\n",
      "[0.]  :  [4.432e-12]\n",
      "[0.]  :  [1.13391005e-10]\n",
      "[0.]  :  [2.73e-11]\n",
      "[0.]  :  [6.87687e-10]\n",
      "[0.]  :  [7.353e-12]\n",
      "[0.]  :  [3.34e-13]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [1.4e-14]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [1.0116e-11]\n",
      "[0.]  :  [2.56e-12]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [5.3e-14]\n",
      "[0.]  :  [2.e-15]\n",
      "[0.]  :  [3.8735412e-05]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [3.0000001e-15]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n",
      "[0.]  :  [0.]\n"
     ]
    }
   ],
   "source": [
    "for (index, val) in enumerate(y_test[10:]):\n",
    "    print(val,\" : \",predictions[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
